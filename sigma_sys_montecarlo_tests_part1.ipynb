{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Tests on Sigma Sys\n",
    "# Here sigma_sys is an overall amplitude offset as described in the paper\n",
    "\n",
    "1) test recovery of Sigma-sys\n",
    "2) see what value Sigma_sys corresponds to (it isn't just a straight average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : test sigma-sys recovery scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0692234,  0.122452 ,  0.216609 ,  0.383168 ,  0.6778   ,\n",
       "        1.19898  ,  2.12093  ,  3.75178  ,  6.6366701, 11.7398005])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The HOD model file\n",
    "# see interpolate.pro\n",
    "modelfile = \"hod_model_saito2016_mdpl2.txt\"       # model for weighting\n",
    "temp = np.genfromtxt(modelfile)\n",
    "model = temp[:,1]\n",
    "rbins = temp[:,0]\n",
    "rbins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallr ,= np.where(rbins<0.1) # 1 Mpc split\n",
    "\n",
    "larger ,= np.where(rbins>1.)\n",
    "allr ,= np.where(rbins<50)\n",
    "\n",
    "binranges = []\n",
    "binranges.append(smallr)\n",
    "binranges.append(larger)\n",
    "binranges.append(allr)\n",
    "\n",
    "nrbin=len(rbins)\n",
    "smallr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes amplitudes\n",
    "\n",
    "def amplitudes(measurements, covariances, model, binranges, experiments, file1, file2, file3, file4):\n",
    "    # measurements is a dictionary of Nd vectors, \n",
    "    # covariances is a dictionary of NxN matrixes, \n",
    "    # model is a Nd vector, \n",
    "    # binranges is a list of integer vectors, \n",
    "    # experiments is a list of names \n",
    "    \n",
    "    for binrange in binranges:\n",
    "        \n",
    "        #print(\"------ Bin range -----\",binrange)\n",
    "        #print(\"bin range \",binrange)\n",
    "        \n",
    "        # pick relevant entries from measurements, covariances, model\n",
    "        meas = {}\n",
    "        cov = {}\n",
    "        invcov = {}\n",
    "        weight = {} # optimal weight for that survey\n",
    "        denom  = {} # the denominator for that survey that brings A=1 if meas=mod\n",
    "        mod = model[binrange]\n",
    "        #print(\"model\",mod)\n",
    "        \n",
    "        # sum of covariances of that set of bins\n",
    "        sumcov = np.zeros((len(binrange),len(binrange)))\n",
    "        \n",
    "        #print(\"\\n----- Calculating weights -------\\n\")\n",
    "            \n",
    "        for experiment in experiments:\n",
    "            \n",
    "            #print(experiment)\n",
    "            meas[experiment] = measurements[experiment][binrange]\n",
    "            cov[experiment]  = covariances[experiment][binrange][:,binrange]\n",
    "            invcov[experiment] = np.linalg.inv(cov[experiment])\n",
    "            weight[experiment] = np.dot(invcov[experiment],mod)\n",
    "            #print(\"experiment\",experiment)\n",
    "            #type(weight[experiment])\n",
    "            #print(\"weight\",weight[experiment])\n",
    "            \n",
    "            denom[experiment] = np.dot(weight[experiment],mod)\n",
    "            \n",
    "            # sum all covs for joint weighting\n",
    "            sumcov += cov[experiment]\n",
    "            \n",
    "        # optimal weight for comparison is inverse of sum of covariances times model\n",
    "        cweight = np.dot(np.linalg.inv(sumcov),mod)\n",
    "        cdenom   = np.dot(cweight,mod) # normalizing the amplitude to 1 if measurement = model\n",
    "        \n",
    "        #print(\"cweight\",cweight)\n",
    "        #print(\"cdenom\",cdenom)      \n",
    "        \n",
    "        #print(\"\\n----- Calculating amplitudes -------\\n\")\n",
    "        \n",
    "        for experiment in experiments:\n",
    "            a    = np.dot(meas[experiment],cweight)/cdenom\n",
    "            siga = np.sqrt(np.dot(np.dot(cweight,cov[experiment]),cweight)/cdenom**2)\n",
    "            #print(experiment,a,\"+-\",siga)\n",
    "            \n",
    "            # Write out amplitudes\n",
    "            file1.write('%0.4f' % a)\n",
    "            file1.write(' ')\n",
    "\n",
    "            # Write out errors\n",
    "            file2.write('%0.4f' % siga)\n",
    "            file2.write(' ')\n",
    "        \n",
    "            #print(\"a and siga\", a, siga)\n",
    "   \n",
    "            #print(\"with its own optimal weighting the experiment\",experiment,\"would have given us\",a,\"+-\",siga)\n",
    "            a    = np.dot(meas[experiment],weight[experiment])/denom[experiment]\n",
    "            siga = np.sqrt(np.dot(np.dot(weight[experiment],cov[experiment]),weight[experiment])/denom[experiment]**2)\n",
    "   \n",
    "        # also print out the weights\n",
    "        # to check if they have imporatnt variations with R\n",
    "        #print('cweight',cweight)\n",
    "        \n",
    "        for item in cweight:\n",
    "            file3.write(\"%0.4f\" % item)\n",
    "            file3.write(' ')\n",
    "            \n",
    "        file4.write('%0.4f' % cdenom)\n",
    "                \n",
    "        file1.write('\\n')\n",
    "        file2.write('\\n')\n",
    "        file3.write('\\n')\n",
    "        file4.write('\\n')\n",
    "            \n",
    "    file1.close()  \n",
    "    file2.close() \n",
    "    file3.close()\n",
    "    file4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\"e1\",\"e2\",\"e3\",\"e4\",\"e5\",\"e6\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide data and errors, and sigma_sys, return reduced chi2-1 with weighed mean\n",
    "# Data and errors passed as globals\n",
    "def computechi2minus1(sigs):\n",
    "    \n",
    "    global data2\n",
    "    global errs2\n",
    "    \n",
    "    n=len(data2)\n",
    "    \n",
    "    # inverse variance mean\n",
    "    w = np.zeros(n)\n",
    "    for i in range(0, n):\n",
    "        w[i]=1.0/(errs2[i]**2)\n",
    "    \n",
    "    wmean = np.average(data2, weights=w)\n",
    "    \n",
    "    #Compute reduced chi2\n",
    "    chi2=0.0\n",
    "\n",
    "    for i in range(0, n):\n",
    "        chi2+=((data2[i]-wmean)**2)/(errs2[i]**2+sigs**2)\n",
    "    \n",
    "    rchi2 = chi2/(n-1)\n",
    "\n",
    "    # Reduced chi2 -1\n",
    "    return rchi2-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connection between error on amplitude and systematic error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Monte Carlo Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change input parameters to run new test\n",
    "# One common statistial error assumed here\n",
    "\n",
    "# Staterr = 0.05   # Statistial in percentage\n",
    "# Systematic error in percentage, but this is one overall \n",
    "# offset (same per radial bin)\n",
    "#    syserr = 0.1\n",
    "\n",
    "def runnewtest(staterr,syserr):\n",
    "\n",
    "    measurements = {}\n",
    "    covariances = {}\n",
    "\n",
    "    k=0    \n",
    "    for experiment in experiments:\n",
    "    \n",
    "        #Data Vector\n",
    "        modelfile = \"hod_model_saito2016_mdpl2.txt\"       # model for weighting\n",
    "        temp = np.genfromtxt(modelfile)\n",
    "        dstemp = temp[:,1]\n",
    "    \n",
    "        # Generate cov that has **statistical** only\n",
    "        covtemp = np.genfromtxt(\"SDSS_LOWZ_0.15_0.31_wtot_cov.dat\")\n",
    "        for i in range(0,nrbin): \n",
    "                for j in range(0,nrbin): \n",
    "                    if (i == j):\n",
    "                        covtemp[i,i] = dstemp[i]*staterr # error in percentage\n",
    "                    else:\n",
    "                        covtemp[i,j]=0.0\n",
    "        #print(covtemp)\n",
    "        #print(\"----\")\n",
    "    \n",
    "        # Now add statistical and systematic error to data vecor\n",
    "        # Statistical is drawn each time anew for each radial bin\n",
    "        # systematic is an offset that is the same in all bins\n",
    "        \n",
    "        this_sys_draw = np.random.normal(0,syserr,1)[0]\n",
    "        \n",
    "        for i in range(0,nrbin): \n",
    "\n",
    "            #and statistical error \n",
    "            sig = dstemp[i]*staterr\n",
    "            \n",
    "            # systematic offset \n",
    "            dstemp[i] = dstemp[i] + dstemp[i]*this_sys_draw \n",
    "            \n",
    "            # Statistical, new draw for each bin\n",
    "            dstemp[i] = dstemp[i]+np.random.normal(0,sig,1)[0]\n",
    "       \n",
    "        \n",
    "        #print(model)\n",
    "        #print(dstemp)\n",
    "        #print(\"----\")\n",
    "    \n",
    "        covariances[experiment] = covtemp\n",
    "        measurements[experiment]= dstemp\n",
    "    \n",
    "        k=k+1\n",
    "    \n",
    "    # Now run the Amplitudes\n",
    "    file1 = open(\"testamplitudes.txt\",'w')\n",
    "    file2 = open(\"testamplitudes_errs.txt\",'w')\n",
    "    file3 = open(\"testamplitudes_cweight.txt\",'w')\n",
    "    file4 = open(\"testamplitudes_cdenom.txt\",'w')\n",
    "    \n",
    "    amplitudes(measurements,covariances,model,binranges,experiments,file1,file2,file3,file4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runnewtest(staterr,syserr):\n",
    "runnewtest(0.05,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change numbers in runnewtest to check different combinations\n",
    "\n",
    "ntest=5000\n",
    "\n",
    "sigest = np.zeros(ntest)\n",
    "\n",
    "for k in range(0,ntest):   # Run the test many many times\n",
    "    \n",
    "    runnewtest(0.05,0.1)\n",
    "    allamp = np.loadtxt(\"testamplitudes.txt\")\n",
    "    allamperrs = np.loadtxt(\"testamplitudes_errs.txt\")\n",
    "    \n",
    "    # All Radial range used here\n",
    "    # Change here to use a different radial range\n",
    "    #print(\"----\")\n",
    "    data=allamp[0]\n",
    "    errs=allamperrs[0]\n",
    "    \n",
    "    n=len(data)\n",
    "    data2 = np.zeros(n)\n",
    "    errs2 = np.zeros(n)\n",
    "    \n",
    "    w = np.zeros(n)\n",
    "    for i in range(0, n):\n",
    "        w[i]=1.0/(errs[i]**2)\n",
    "    \n",
    "    wmean = np.average(data, weights=w) \n",
    "    meanamp = np.average(data, weights=w) \n",
    "        \n",
    "    #Compute reduced chi2\n",
    "    chi2=0.0\n",
    "\n",
    "    for i in range(0, n):\n",
    "        chi2+=((data[i]-wmean)**2)/(errs[i]**2)\n",
    "    \n",
    "    rchi2 = chi2/(n-1)\n",
    "    #print(\"Reduced chi2: \", rchi2)\n",
    "    \n",
    "    meanstaterr = np.mean(errs)\n",
    "    \n",
    "    # If reduces chi2 >1 then proceed \n",
    "    if (rchi2>1):\n",
    "        \n",
    "        # Find the value that makes rchi2=1\n",
    "        # Set global variables\n",
    "        for i in range(0, n):\n",
    "            data2[i]=data[i]\n",
    "            errs2[i]=errs[i]\n",
    "\n",
    "        sol=optimize.root_scalar(computechi2minus1, bracket=[0,2.0])\n",
    "        sigmasys=sol.root\n",
    "        sigest[k]=sigmasys\n",
    "              \n",
    "    # --------------------------     \n",
    "    # Confidence interval for sigma_sys\n",
    "    # this is a frequentist thing -- at what values of sigma_sys \n",
    "    # (including 0) do the differences we find between the surveys produce a chi^2 that is within the central\n",
    "    # one and two sigma of the distribution\n",
    "    #--------------------------\n",
    "\n",
    "    ndof = len(data)-1 # should be chi^2-distributed with this ndof, since fitting for mean\n",
    "\n",
    "    Dsigma_sys = np.arange(0,0.6,0.01) # range of sigma_sys to probe\n",
    "\n",
    "    Dchisq = np.zeros(len(Dsigma_sys)) \n",
    "    # the chi^2 we would have if we included sigma_sys mutually independent systematic error\n",
    "    for i in range(len(Dsigma_sys)):\n",
    "        Dchisq[i] = np.sum((data-wmean)**2/(errs**2+(Dsigma_sys[i])**2))\n",
    "    \n",
    "    # ---- 1 Sigma Limits -----\n",
    "    # inverse of 1-CDF \n",
    "    xx=(1.0-0.68)/2.0 # 68 confidence\n",
    "    # inverse survival function\n",
    "    limits = scipy.stats.chi2.isf(q=[1.0-xx,xx],df=ndof)\n",
    "    #print(\"68% confidence interval for chi^2 (in chi2 space) is\", limits)\n",
    "\n",
    "    # Confidene interval for the observed chi2\n",
    "    ok ,= np.where(np.logical_and(Dchisq>limits[0],Dchisq<limits[1]))\n",
    " \n",
    "\n",
    "    #print(\"68% confidence interval on sigma sys is\",Dsigma_sys[ok[0]],Dsigma_sys[ok[-1]])\n",
    "        \n",
    "#plt.hist(sigest,bins=20,range=[0,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=1000\n",
    "plt.hist(sigest,bins=20,range=[0,0.30])\n",
    "#plt.plot([sigsys,sigsys],[0,y])\n",
    "plt.plot([np.mean(sigest),np.mean(sigest)],[0,y])\n",
    "plt.plot([np.median(sigest),np.median(sigest)],[0,y])\n",
    "#sigest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
